{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A6 - sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmiXo913oFBHiabJ0lrI5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjamin-carter/NLP---preproc/blob/master/A6_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLn3-GrkZ-oj",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 6: Twitter pre-processing and catgeorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCX3y5uRaF3Y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Connect to Google Drive to access the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwg1MKr2Z9Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0hvt0dZ-Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls \"/content/gdrive/My Drive/Current\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh9GYK_nqUw1",
        "colab_type": "text"
      },
      "source": [
        "## Add necessary packages for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADgu1ttufsGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  # Returns split words, while tf vectorizes after splitting.\n",
        "\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCVy8ODaaMHg",
        "colab_type": "text"
      },
      "source": [
        "## Retrieve Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE1k7Bb9aH_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Current/uglywords_v1.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6EhgZa2qejz",
        "colab_type": "text"
      },
      "source": [
        "## Remove bad symbols and usernames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4AAYAmH8vEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pop_list = ['#', '@', '*', '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷', '~', 'ÛÏ', 'åÊ'\n",
        "            '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', '...', '??', '|', '_', ' - ', 'Ûªs', '', '[', ']', '`', '(', ')', '=', '&amp;', 'Ã©', \n",
        "            \"!!\", \"!?\", \"?!\", 'Ûª', 'ÛÓ', '&', '*', ',', \"'\", \"!\", \"?\", \"├\"]\n",
        "clean_tweets = []\n",
        "for i in range(len(df)):\n",
        "    # max_length = max(max_length, len(df.iloc[i].values[1]))\n",
        "    ws = word_tokenize(df.iloc[i].values[1])\n",
        "    if 'http' in ws:\n",
        "        text = ' '.join(ws[0:ws.index('http')])\n",
        "    elif 'https' in ws:\n",
        "        text = ' '.join(ws[0:ws.index('https')])\n",
        "    else:\n",
        "        text = ' '.join(ws)\n",
        "    temp = word_tokenize(text)\n",
        "    temp2 = []\n",
        "    for i in range(len(temp)):\n",
        "      if temp[i][0] != \"@\":\n",
        "        temp2.append(temp[i])\n",
        "    text = ' '.join(temp2)\n",
        "    for pop in pop_list:\n",
        "        text = text.replace(pop, '')\n",
        "    text = text.split('åÊ')\n",
        "    text = ' '.join(text)\n",
        "    text = [item.lower() for item in text]\n",
        "    text = ''.join(text)\n",
        "    clean_tweets.append(text)\n",
        "df['Tweet'] = clean_tweets\n",
        "df['Tweet'] = df['Tweet'].astype(str) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYrZq617quZ7",
        "colab_type": "text"
      },
      "source": [
        "## Spell Check, Stem content, Spell Check again, and Remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKDFf953f5cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "clean_tweets = []\n",
        "for i in range(len(df)): \n",
        "    ws = word_tokenize(df.iloc[i].values[1])\n",
        "    for j in range(len(ws)): \n",
        "      ws[j] =spell.correction(ws[j])\n",
        "    text = ' '.join(ws)\n",
        "    clean_tweets.append(text)\n",
        "  \n",
        "df['Tweet'] = clean_tweets\n",
        "df['Tweet'] = df['Tweet'].astype(str) \n",
        "########\n",
        "porter_stemmer=PorterStemmer()\n",
        "\n",
        "clean_tweets = [] \n",
        "for i in range(len(df)):\n",
        "  ws = [porter_stemmer.stem(word) for word in word_tokenize(df.iloc[i].values[1])]\n",
        "  text = ' '.join(ws)\n",
        "  clean_tweets.append(text)\n",
        "\n",
        "df['Tweet'] = clean_tweets\n",
        "df['Tweet'] = df['Tweet'].astype(str)\n",
        "\n",
        "clean_tweets = []\n",
        "for i in range(len(df)): \n",
        "    ws = word_tokenize(df.iloc[i].values[1])\n",
        "    for j in range(len(ws)): \n",
        "      ws[j] =spell.correction(ws[j])\n",
        "    text = ' '.join(ws)\n",
        "    clean_tweets.append(text)\n",
        "  \n",
        "df['Tweet'] = clean_tweets\n",
        "df['Tweet'] = df['Tweet'].astype(str) \n",
        "\n",
        "##################\n",
        "\n",
        "clean_tweets = []\n",
        "for i in range(len(df)):\n",
        "    ws = [w for w in word_tokenize(df.iloc[i].values[1]) if w not in stopwords.words('english')]\n",
        "    text = ' '.join(ws)\n",
        "    clean_tweets.append(text)\n",
        " \n",
        "df['Tweet'] = clean_tweets\n",
        "df['Tweet'] = df['Tweet'].astype(str) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJrbUREFq-1X",
        "colab_type": "text"
      },
      "source": [
        "# Code for Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTXXOnurCKH",
        "colab_type": "text"
      },
      "source": [
        "## Converting data into Tensor format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrpoETLVh1io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new = df[:749]\n",
        "df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "df_test = df_new[:99]\n",
        "df_train = df_new[99:]\n",
        "\n",
        "train_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(df_train['Tweet'].values, tf.string),\n",
        "            tf.cast(df_train['Label'].values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "test_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(df_test['Tweet'].values, tf.string),\n",
        "            tf.cast(df_test['Label'].values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUNGrkAmrGTv",
        "colab_type": "text"
      },
      "source": [
        "# Build Pre-Trained Model and set the parameters, optimization, and loss schemes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBwvrpYjhVyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "#train_examples_batch\n",
        "\n",
        "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "#hub_layer(train_examples_batch)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(32,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(.01))\n",
        "model.add(tf.keras.layers.Dense(16, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "#model.summary()\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIKqo7yihTRp",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUJwBT6mhaBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history = model.fit(train_data.shuffle(1000).batch(512), validation_data=test_data.batch(512), epochs = 20, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD5jRYqHhWNG",
        "colab_type": "text"
      },
      "source": [
        "## Display Results of Model with Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9mbgWVThbkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_data.batch(50), verbose = 2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name,value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9EL_ZdlLxtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = df_test.to_numpy()\n",
        "np.sum(temp[:,0])/len(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzOTuFe4hgFh",
        "colab_type": "text"
      },
      "source": [
        "# Code for In-House Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II0NyHeohkeL",
        "colab_type": "text"
      },
      "source": [
        "## Encode Text into Numerical Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwVQfqkihRjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "new_test = ' '.join(clean_tweets)\n",
        "vocab = set(np.array(list(word_tokenize(new_test))))\n",
        "vocab = np.array(list(vocab))\n",
        "values = ' '.join(vocab)\n",
        "values = np.array(values)\n",
        "integer_encoded = label_encoder.fit_transform(vocab)\n",
        "word_dict = {}\n",
        "for i in range(len(vocab)):\n",
        "  word_dict[vocab[i]] = integer_encoded[i]\n",
        "max_length = 0\n",
        "for i in range(len(clean_tweets)):\n",
        "  max_length = max(max_length, len(word_tokenize(clean_tweets[i])))\n",
        "numb_tweets = np.zeros((len(clean_tweets), max_length))\n",
        "print(max_length,numb_tweets.shape)\n",
        "for i in range(len(clean_tweets)):\n",
        "  temp = word_tokenize(clean_tweets[i])\n",
        "  for j in range(len(temp)-1):\n",
        "    numb_tweets[i,j] = word_dict[temp[j]]\n",
        "labels = df['Label'].to_numpy()\n",
        "df_numpy = np.column_stack((labels, numb_tweets))\n",
        "#ones_test = np.ones((99,1))\n",
        "#ones_train = np.ones((639,1))\n",
        "#df_new = df[:738]\n",
        "#df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
        "#nt_new = numb_tweets[:738,:]\n",
        "\n",
        "df_new = df_numpy[:738,:]\n",
        "np.random.shuffle(df_new)\n",
        "df_test = df_new[:99,:]\n",
        "df_train = df_new[99:,:]\n",
        "#nt_test = nt_new[:99,:]\n",
        "#nt_train = nt_new[99:,:]\n",
        "\n",
        "\n",
        "feat_train = df_train[:,1:].astype(int)\n",
        "label_train = df_train[:,0].astype(int)\n",
        "\n",
        "feat_test = df_test[:,1:].astype(int)\n",
        "label_test = df_test[:,0].astype(int)\n",
        "\n",
        "train_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(feat_train, tf.int64),\n",
        "            tf.cast(label_train, tf.int64)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "test_data = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(feat_test, tf.int64),\n",
        "            tf.cast(label_test, tf.int64)\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GBwodWjhs6i",
        "colab_type": "text"
      },
      "source": [
        "## Build Sequential Model and set parameters, optimization and loss schemes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh8K8G_32Izv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(len(vocab)+1, 50, input_length=max_length))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(len(vocab)+1,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(.1))\n",
        "model.add(tf.keras.layers.Dense(500,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(.05))\n",
        "model.add(tf.keras.layers.Dense(100,activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(.01))\n",
        "model.add(tf.keras.layers.Dense(32,activation = 'relu'))\n",
        "#model.add(tf.keras.layers.Dropout(.01))\n",
        "model.add(tf.keras.layers.Dense(16,activation = 'relu'))\n",
        "#model.add(tf.keras.layers.Dropout(.005))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "#keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
        "#model.compile(optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False), loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bkFEa7h2X-",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKLmXDU5b3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_data.shuffle(1000).batch(256), validation_data=test_data.batch(256), epochs = 20, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57yGXoR5h61m",
        "colab_type": "text"
      },
      "source": [
        "## Display Results of Model with Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYBe5eHjb0_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_data.padded_batch(256), verbose = 2)\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name,value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WZCnnCs0gRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum(label_test)/len(label_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}